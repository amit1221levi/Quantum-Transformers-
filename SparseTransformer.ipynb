# ==================
# ==================

#In theory, this model follows multiple architectures in reality so this project on the process and there are some bugs because
# of the randomness of the size of the layer (results of the quantum Layers pass don't have a fixed size).

# ==================
# ==================




# ==================
# IMPORTS
# ==================

import torch
import pennylane as qml
from pennylane.templates import AmplitudeEmbedding
from torch import nn
from torch.nn import functional as F
import spacy
from torch.utils.data import DataLoader
import pennylane as qml
from qiskit import Aer
from qiskit import execute
from sklearn.model_selection import train_test_split
from nltk.corpus import treebank
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD
import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import pennylane as qml
from qiskit import Aer
import nltk
from nltk.corpus import treebank
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
from qiskit import QuantumRegister, ClassicalRegister, QuantumCircuit, transpile, assemble, execute
from qiskit.visualization import plot_bloch_multivector, plot_histogram
import spacy
from sklearn.model_selection import train_test_split
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import numpy as np
import nltk
from nltk.corpus import treebank
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
from qiskit import QuantumRegister, ClassicalRegister
from qiskit import QuantumCircuit, transpile, assemble, Aer, execute
from qiskit.visualization import plot_bloch_multivector, plot_histogram


# ==================
# DATA LOADING AND PREPARATION
# ==================

class TreeBankDataset(Dataset):
    def __init__(self, sentences, tokenizer, vocab):
        self.sentences = sentences
        self.tokenizer = tokenizer
        self.vocab = vocab

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        tokenized_sentence = self.tokenizer(self.sentences[idx])
        numericalized_sentence = [self.vocab.stoi[t] for t in tokenized_sentence]
        return torch.tensor(numericalized_sentence)




# ==================
# UTILS
# ==================

# Define tokenizer
def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

# Define the quantum circuit
def create_circuit(x):
    qr = QuantumRegister(2)
    cr = ClassicalRegister(2)
    qc = QuantumCircuit(qr, cr)

    for i in range(len(x)):
        if x[i] == 1:
            qc.x(qr[i])

    qc.h(qr)
    qc.measure(qr, cr)

    return qc

# Define the Hadamard test
def hadamard_test(x):
    qc = create_circuit(x)
    backend = Aer.get_backend('qasm_simulator')
    job = execute(qc, backend, shots=1000)
    result = job.result()
    counts = result.get_counts(qc)
    p0 = counts['00'] / 1000

    return p0

def sliding_chunks_matmul_qk(q: torch.Tensor, k: torch.Tensor, window: int):
    b, h, n, _, dtype, device = *q.size(), q.dtype, q.device
    k = F.pad(k, (0, 0, window - 1, window - 1))  
    qk = torch.einsum("bhid,bhjd->bhij", q, k)
    qk = F.unfold(qk, (window, n), stride=1) 
    return qk

def sliding_chunks_matmul_pv(p: torch.Tensor, v: torch.Tensor, window: int):
    b, h, n, _, dtype, device = *p.size(), p.dtype, p.device
    v = F.pad(v, (0, 0, 0, window - 1))
    pv = torch.einsum("bhoi,bhdi->bhod", p, v)
    return pv.contiguous()

# ==================
# CLASSES
# ==================

class QuantumLayer(torch.nn.Module):
    #ValueError: ---> 11     qml.templates.AmplitudeEmbedding(inputs, wires=range(self.n_qubits), normalize=True)
 #Features must be a one-dimensional tensor; got shape (dynamic! size, 100).

    def __init__(self, n_qubits, n_features):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_features = n_features
        self.qnode = qml.QNode(self.circuit, qml.device('default.qubit', wires=self.n_qubits))
        self.fc = torch.nn.Linear(self.n_qubits, 1)


    def circuit(self, inputs):
        qml.templates.AmplitudeEmbedding(inputs, wires=range(self.n_qubits), normalize=True)
        qml.templates.StronglyEntanglingLayers(inputs, wires=range(self.n_qubits))
        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]
    
    def forward(self, x):
        batch_size, seq_length, embedding_dim = x.shape
        out = []
        for i in range(batch_size):
            example = x[i, :, :]
            example = example.flatten()  # or example.view(-1)
            q_out = self.circuit(example)
            out.append(q_out)
        return torch.stack(out)
    
    #ValueError: Features must be a one-dimensional tensor; got shape (50, 100).


# Simple transformer model
class SimpleTransformer(torch.nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, n_qubits, n_features, num_classes):
        super(SimpleTransformer, self).__init__()
        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)
        self.quantum_layer = QuantumLayer(n_qubits, n_features)
        self.fc = torch.nn.Linear(n_qubits, num_classes)
        # SparseAttention(nn.Module)
        AdaptiveSpan(nn.Module)
        SparseTransformer(nn.Module)
        



    
    def forward(self, x):
        x = self.embedding(x)
        x = self.quantum_layer(x)
        x = self.fc(x)
        return x



class SparseAttention(nn.Module):
    def __init__(self, attention_window, hidden_size, num_heads):
        super().__init__()
        self.attention_window = attention_window
        self.hidden_size = hidden_size
        self.num_heads = num_heads

    def forward(self, query, key, value, mask=None):
        batch_size, seq_len, _ = query.size()
        attn_span = self.adaptive_span()

        if seq_len % (attn_span * 2) != 0:
            raise ValueError("Sequence length must be a multiple of attention window size")

        def reshape(input_tensor):
            return input_tensor.view(batch_size, seq_len, self.num_heads, self.hidden_size // self.num_heads).transpose(1, 2)

        query, key, value = map(reshape, (query, key, value))

        if mask is not None:
            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)

        attn_output_weights = sliding_chunks_matmul_qk(query, key, attn_span, padding_value=0, mask=mask)
        attn_output_weights = F.softmax(attn_output_weights, dim=-1)

        attn_output = sliding_chunks_matmul_pv(attn_output_weights, value, attn_span)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)

        return attn_output 
    
class AdaptiveSpan(nn.Module):
    def __init__(self, attention_window, max_span, parametrize="softplus", scale=1.):
        super().__init__()
        self.attention_window = attention_window
        self.max_span = max_span

        self.scale = scale
        self._lambda = nn.Parameter(torch.zeros(()))

        self.parametrize = parametrize

    def forward(self):
        span = self.scale * self.parametrization()
        span = torch.clamp(span, 0, float(self.max_span))
        return span.round().long()

    def parametrization(self):
        if self.parametrize == "sigmoid":
            return self.attention_window * torch.sigmoid(self._lambda)
        elif self.parametrize == "softplus":
            return self.attention_window * F.softplus(self._lambda)
        else:
            raise NotImplementedError

class SparseTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim, hidden_dim, num_heads, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, embed_dim)
        self.sparse_attn = SparseAttention(attention_window=64, hidden_size=embed_dim, num_heads=num_heads)
        self.fc = nn.Linear(hidden_dim, input_dim)

    def forward(self, x):
        x = self.embedding(x)
        x = self.sparse_attn(x, x, x)
        x = self.fc(x)
        return x


def collate_fn(batch):
    lengths = [len(sent) for sent in batch]
    max_length = max(lengths)

    # Pad sentences to max_length
    batch = [sent + [vocab['<PAD>']]*(max_length - len(sent)) for sent in batch]
    
    return torch.LongTensor(batch)


# VISUALIZATION
# ==================

# Plot word embeddings
def plot_embeddings(model, word_to_ix):
    word_list = list(word_to_ix.keys())
    word_embeddings = get_word_embeddings(model, word_list)
    word_embeddings_2d = TSNE(n_components=2).fit_transform(word_embeddings)

    sns.set(rc={'figure.figsize':(11.7,8.27)})
    sns.scatterplot(word_embeddings_2d[:,0], word_embeddings_2d[:,1])
    for i, word in enumerate(word_list):
        plt.annotate(word, xy=(word_embeddings_2d[i,0], word_embeddings_2d[i,1]))
    plt.show()

# Get word embeddings
def get_word_embeddings(model, word_list):
    word_embeddings = []
    for word in word_list:
        word_idx = torch.tensor([word_to_ix[word]])
        word_embedding = model.embedding(word_idx).detach().numpy()[0]
        word_embeddings.append(word_embedding)
    return np.array(word_embeddings)


# Download and load the Penn Treebank dataset
nltk.download('treebank')
data = treebank.words()

# Select a subset of data for demonstration
data = data[:100]
sentences = [' '.join(data[i:i+10]) for i in range(0, len(data), 10)]

# Perform text embeddings with CountVectorizer and TruncatedSVD
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(sentences)
svd = TruncatedSVD(n_components=2)  # Reduce to 2 dimensions for simplicity
X = svd.fit_transform(X.toarray())

# Normalize vectors
X = X / np.linalg.norm(X, axis=1, keepdims=True)

# Set up the quantum circuit
q = QuantumRegister(2)
c = ClassicalRegister(2)
qc = QuantumCircuit(q, c)

# Initialize the quantum circuit with the embedding data
for i in range(2):
    qc.initialize(list(X[i]), i)

# Transpile the circuit for the simulator
qc = transpile(qc)

# Simulate the quantum circuit
simulator = Aer.get_backend('statevector_simulator')
job = execute(qc, simulator)
result = job.result()

# Extract and visualize the final state
statevector = result.get_statevector(qc)
print(statevector)

plot_bloch_multivector(statevector)


# Data preparation
class TreeBankDataset(Dataset):
    def __init__(self, sentences, tokenizer, vocab):
        self.sentences = sentences
        self.tokenizer = tokenizer
        self.vocab = vocab

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        tokenized_sentence = self.tokenizer(self.sentences[idx])
        numericalized_sentence = [self.vocab.stoi[t] for t in tokenized_sentence]
        return torch.tensor(numericalized_sentence)

def collate_fn(batch):
    lengths = [len(sent) for sent in batch]
    max_length = max(lengths)
    batch = [sent + [vocab['<PAD>']]*(max_length - len(sent)) for sent in batch]
    return torch.LongTensor(batch)

# Load and process data
nltk.download('treebank')
sentences = [' '.join(sent) for sent in treebank.sents()]
spacy_en = spacy.load('en_core_web_sm')
tokenize_en = lambda text: [tok.text for tok in spacy_en.tokenizer(text)]
tokenized_sentences = [tokenize_en(sent) for sent in sentences]
word_counter = Counter()
for sent in tokenized_sentences:
    word_counter.update(sent)
vocab = {word: i+2 for i, (word, _) in enumerate(word_counter.most_common())}
vocab['<PAD>'] = 0
vocab['<UNK>'] = 1
train_sentences, test_sentences = train_test_split(tokenized_sentences, test_size=0.1)
train_data = [[vocab.get(token, vocab['<UNK>']) for token in sent] for sent in train_sentences]
test_data = [[vocab.get(token, vocab['<UNK>']) for token in sent] for sent in test_sentences]
train_iter = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)
test_iter = DataLoader(test_data, batch_size=32, collate_fn=collate_fn)


# Model, optimizer and loss
model = SimpleTransformer(len(vocab), 100, 8, 2, 2, len(vocab))
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()



# Training loop
model.train()
for epoch in range(10):
    for i, batch in enumerate(train_iter):
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output.view(-1, len(vocab)), batch.view(-1))
        loss.backward()
        optimizer.step()
        if i % 100 == 0:
            print(f'Epoch: {epoch}, Loss: {loss.item()}')

    

# Evaluation loop
model.eval()
with torch.no_grad():
    for i, batch in enumerate(test_iter):
        output = model(batch)
        loss = criterion(output.view(-1, len(vocab)), batch.view(-1))
        if i % 100 == 0:
            print(f'Loss: {loss.item()}')


# ==================
# Visualization
embeddings = model.embedding.weight.detach().numpy()
svd = TruncatedSVD(n_components=2)
reduced_embeddings = svd.fit_transform(embeddings)
plt.figure(figsize=(20, 20))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
for i, txt in enumerate(vocab):
    plt.annotate(txt, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))
plt.show()


